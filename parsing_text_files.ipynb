{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: 01/09/2021\n",
    "\n",
    "Version: 4.0\n",
    "\n",
    "Environment: Python 3.8.3 and Anaconda 6.0.3\n",
    "\n",
    "Operating System: macOS Big Sur (Version 11.5.1)\n",
    "\n",
    "Libraries used:    \n",
    "* re (for regular expressions)\n",
    "* os (for file/directory related operations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:44:12.438032Z",
     "start_time": "2021-09-03T12:44:12.432996Z"
    }
   },
   "source": [
    "## Table of Contents\n",
    "* 1. Introduction\n",
    "* 2. Import libraries\n",
    "* 3. Examining and loading data\n",
    "* 4. Helper Functions\n",
    "* 5. Segregating each user instance\n",
    "* 6. Regexes\n",
    "* 7. Parsing and Operations\n",
    "* 8. Creating XML string\n",
    "* 9. Writing XML file\n",
    "* Conclusion\n",
    "* References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T09:35:49.913818Z",
     "start_time": "2021-09-03T09:35:49.842578Z"
    }
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This task assesses our ability to extracting data from our designated semi-structured text file. The dataset is about cryptocurrency tweets with various metadata involved.\n",
    "\n",
    "Each text file contains information about the tweets, i.e., `â€œuser nameâ€`, `â€œuser codeâ€`, `â€œuser descriptionâ€`, `â€œnumber of followersâ€`, `â€œwhether or not the user account is verifiedâ€`, `â€œdate of the tweetâ€`, and the `â€œtweet textâ€`. \n",
    "\n",
    "Our task is to extract the data from the text file and transform the data into a `XML format` with the following elements:\n",
    "\n",
    "1. users: this tag wraps all the users\n",
    "2. user: this tag wraps all the tweets from a particular user and keeps the meta data for each user such as number of followers, verified or not, user description etc. If a user has multiple tweets, the meta data of the latest tweet (i.e., the tweet with the most recent date) must be used.\n",
    "3. Tweets: wraps all the tweets of a specific user\n",
    "4. tweet: for each user, this tag represents the text of the user tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:50:26.062106Z",
     "start_time": "2021-09-03T12:50:26.058790Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examining and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:50:26.106558Z",
     "start_time": "2021-09-03T12:50:26.065633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"$user_name.: Sedekah Bang\\n$user_code.: 100024962\\n$user_desc.: cari dolar\\n$No. followers.: 1.0 $verified_user?.: False $tweet_date.: 2021-06-23 17:52:12\\n$tweet.: I found #bitcoin in a User vault at this location! Join me playing #coinhuntworld, It's awesome! https://t.co/xBy6ZGO8jZ #cryptocurrency #14303 https://t.co/ZXLt62F6wr\\n$uname.: Harshen Hars\\n$user_code.: 100000003\\n$tweet_date.: 2021-02-19 22:05:02\\n$tweet.: $BTC A big chance in a million! Price: \\\\6114862.0 (2021/02/20 07:04) #Bitcoin #FX #BTC #crypto\\n$verified_user?.: False $followerNo.: 648.0 $user_desc.: nan\\n$username.: ðŸŒ‹ SmallState HODL ðŸŒ‹\\n$user_code.: 100035233\\n$udesc.: #bitcoin\\n$No. followers.: 1368.0 $verified_user?.: False $tweet_date.: 2021-06-22 04:41:08\\n$tweet_text.: This is why we #bitcoin https://t.co/CjAiwNE1eR\\n$user_name.: aya | ia â™¨\\n$user_code.: 100061387\\n$udesc.: @BTS_twt || #JanielGang || #EunieTy || #ThatGiveawaySquid || #BaniousRaid || $EDDA\\n\\nyou are sky that full of star âœ¨ðŸŒŒ\\n$No. followers.: 480.0 $verified?.: False $tweet_date.: 2021-06-21 11:59:28\\n$tweet.: #BTC Portugal Grants First Crypto Exchanges Operating Licenses #Bitcoin\\n$username.: DavidNguyen\\n$user_code.: 100073421\\n$userdescription.: Waves Wallet\\n3PJ8z4Sd4PPZpx5bf14hZ5fR2i2wfm6PfrC\\n$followerNo.: 74.0 $verified_user?.: False $tweet_date.: 2021-07-04 10:17:59\\n$tweet.: 1 day left #Galaxyprotocol #Galaxia #safegalaxy @SafeGalaxyToken @GalaxySwapX \\n\\n#BSCGems #BSC #cryptocurrency #CryptoNews #Cryptocurency #Crypto #BNB #doge #Bitcoin #ETH #staking #altcoins #altcoingems https://t.co/LVLCn3QJ9X\\n$uname.: Tomato-crypto Shitcoin Shill\\n$user_code.: 100010453\\n$user_desc.: Shitcoin + Bitcoin billionaire to French Fry cook at McDanks. Margin call ninja. King of leverage - I took out 4 mortgages on the same house before going broke.\\n$followerNo.: 66.0 $verified_user?.: False $tweet_date.: 2021-02-19 22:02:31\\n$tweet.: Don't you know pump it up ?\\n\\n#btc #bitcoin\\n$user_name.: ENERGYGAMBLER\\n$user_code.: 100048213\\n$user_desc.: Hi.. mathematics gambler.. President is my president! Oil Gas Energy Solar â€œDonâ€™t like wind - ugly - I luv birdsâ€\\n$followerNo.: 30.0 $verified_user?.: False $tweet_date.: 2021-07-05 16:46:18\\n$tweet_text.: Institutions will block you from buying #Bitcoin, so thereâ€™s more for them\\n$user_name.: Mehidi Hasan\\n$user_code.: 100030713\\n$udesc.: nan\\n$followerNo.: 2022.0 $verified_user?.: False $tweet_date.: 2021-07-05 16:46:09\\n$tweet.: @IoLunaland Great project with good performance. anyone can get success. I am glad about the rapid development of the project.\\n#presale #blockchain #cryptotrading #investing #cryptocurrency #bitcoin #trading #altcoin #cryptonews\\n$username.: keals\\n$user_code.: 100002050\\n$user_desc.: nan\\n$followerNo.: 69.0 $verified?.: False $tweet_date.: 2021-04-19 18:53:14\\n$tweet_text.: The all time high price of Bitcoin is $64,863.1\\nIt was on April 14, 2021 (5 days ago).\\n#ATH #AllTimeHigh #BTC #Bitcoin\\nhttps://t.co/7XxSMXZVsC\\n$uname.: 'â“Ÿ Bilâ­•ï¸uBil crypto F ðŸ…¡â€™\\n$user_code.: 100000104\\n$userdescription.: crypto maniac... gem hunterðŸš€ðŸš€ $OMI $VRA $RSR $CNS $AXION $SNTVT $KAI\\n$No. followers.: 6640.0 $verified?.: False $tweet_date.: 2021-04-07 13:15:26\\n$tweet.: ðŸ’µ1,319 #BTC (76,514,800 USD) move from multiple addresses to #poloniex\\n \\nDate : 2021-04-07 13:13:51 (GMT 0)\\nBlockchâ€¦ https://t.co/wcXSBTh9ts\\n$uname.: ð•Šð•¥ð•–ð•§ð•–ð•Ÿ ð”¹ð•£ð•šð•’ð•Ÿ\\n$user_code.: 100000026\\n$userdescription.: â„‚ð•£ð•ªð•¥ð•¡ð•  ð”¸ð•Ÿð•’ð•ð•ªð•¤ || â„‚ð•£ð•ªð•¡ð•¥ð•  â„™ð•’ð•¤ð•¤ð•šð•§ð•– ð•‹ð•£ð•’ð••ð•–ð•£ || ð”¸ð•ð•¥ð•”ð• ð•šð•Ÿ ð•ƒð• ð•§ð•–ð•£ð•¤         ~ð‘€ð’¶ð“€ð‘’ ð“Žð‘œð“Šð“‡ ð’¹ð’¶ð“Ž ð“‰ð‘œ ð’·ð‘’ ð’½ð’¶ð“…ð“…ð“Ž\\n$No. followers.: 872.0 $verified?.: False $tweet_date.: 2021-02-10 23:40:48\\n$tweet.: #BTC/USD 4H. #Bitcoin consolidating between support ~$43k and resistance ~$47.5k. Stop loss should be ~$42.5k and eâ€¦ https://t.co/Hd00WT83WE\\n$username.: Luciano Iturbe\\n$user_code.: 100022097\\n$userdescription.: #Bitcoin\\n$followerNo.: 92.0 $verified?.: False $tweet_date.: 2021-07-03 12:54:58\\n$tweet_text.: @litecoinmini Thanks for the opportunity, big chance for winner\\nI hopefully in the list of winner and success for this airdrop ðŸŽ‰\\n\\n@AkunSmurf0 \\n@Tothemoon171 \\n@rej\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('31072100_task1_input.txt','r') as tweet_file:\n",
    "    read_tweets = tweet_file.read()\n",
    "\n",
    "# checking first 4000 chars for examination/observation    \n",
    "read_tweets[:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:50:26.123663Z",
     "start_time": "2021-09-03T12:50:26.110141Z"
    }
   },
   "outputs": [],
   "source": [
    "def datetime_split(date):\n",
    "    \"\"\"\n",
    "     take date including data-time as input and returns \n",
    "     year, month, day, hour, minute, seconds values\n",
    "     :param date: date-time in designated format. Eg 2021-04-18 13:48:25\n",
    "    \"\"\"\n",
    "    date_part = date.split(' ')[0]\n",
    "    time_part = date.split(' ')[1]\n",
    "    \n",
    "    year, month, day = map(int, date_part.split('-'))\n",
    "    hour, minute, sec = map(int, time_part.split(':'))\n",
    "    \n",
    "    return (year, month, day, hour, minute, sec)\n",
    "\n",
    "\n",
    "def compare_datetime(date1, date2):\n",
    "    \"\"\"\n",
    "     take two date-time parameters as input and returns true if \n",
    "     2nd date-time is higher than 1st, False otherwise\n",
    "     :param date1: 1st date-time\n",
    "            date2: 2nd date-time \n",
    "    \"\"\"\n",
    "    year1, month1, day1, hour1, minute1, sec1 = datetime_split(date1)\n",
    "    year2, month2, day2, hour2, minute2, sec2 = datetime_split(date2)\n",
    "        \n",
    "    return (year2, month2, day2, hour2, minute2, sec2) > (year1, month1, day1, hour1, minute1, sec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:50:26.154424Z",
     "start_time": "2021-09-03T12:50:26.149455Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing code\n",
    "date1 = '2021-04-18 13:48:25'\n",
    "date2 = '2021-02-06 16:48:35'\n",
    "\n",
    "compare_datetime(date1, date2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `maketrans()` method returns a mapping table that can be used with the `translate()` method to replace specified characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:50:26.161002Z",
     "start_time": "2021-09-03T12:50:26.156989Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def handle_xml_chars(value, tag_type):\n",
    "    \"\"\"\n",
    "      translates and replaces the value provided as dictionary according to tag_type\n",
    "      :param value\n",
    "             tag_type \n",
    "    \"\"\"\n",
    "    if tag_type == 'xml_description' or tag_type == 'xml_tweet':\n",
    "        return value.translate(str.maketrans({'&': '&amp;', '<':'&lt;','>':'&gt;'}))\n",
    "    if tag_type == 'xml_name':\n",
    "        return value.translate(str.maketrans({'\"': '&quot;', '&': '&amp;'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Segregating each user instance\n",
    "\n",
    "We will segregate each user instance containing meta data and tweet to get the each user instances of tweet. We write a regex to achieve this and then use `findall()` so as to get each instance as an element of list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:50:26.319585Z",
     "start_time": "2021-09-03T12:50:26.163557Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12129\n"
     ]
    }
   ],
   "source": [
    "# matches each user data instance\n",
    "tweet_info_regex = re.compile(\n",
    "                        \"(?s)\"\n",
    "                        \"(?:\\$(?:u(?:ser)?_?name)\\.:)\"\n",
    "                        \".*?\"\n",
    "                        \"(?=\\$(?:u(?:ser)?_?name)|$)\"\n",
    "                    )\n",
    "\n",
    "tweet_info = re.findall(tweet_info_regex, read_tweets)\n",
    "print(len(tweet_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:45:10.802619Z",
     "start_time": "2021-09-03T12:45:10.798159Z"
    }
   },
   "source": [
    "> We got `12129` user instances/blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regex Explanation**\n",
    "\n",
    "In the above regex, `(?s)` enables he following effective flags: `gs`, where `s` modifier means single line. Dot matches newline characters too and not ignored. We use non-capturing group `?:` to extract the various variations of \n",
    "`user name` followed by character `.:`.\n",
    "\n",
    "The data we capture is captured using `.*?` with lazy match enabled.\n",
    "\n",
    "The last group is positive lookahead with 2 alternatives. 1st one is same as our `user name` group explained earlier. The 2nd one `$` asserts position at the end of the string, or before the line terminator right at the end of the string (if any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T10:39:34.107771Z",
     "start_time": "2021-09-03T10:39:34.085604Z"
    }
   },
   "source": [
    "## 6. Regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:50:26.327292Z",
     "start_time": "2021-09-03T12:50:26.322696Z"
    }
   },
   "outputs": [],
   "source": [
    "re_uname = re.compile(\"(?:\\$u(?:ser)?_?name\\.:\\s)(.+)\")\n",
    "re_ucode =re.compile(\"(?:\\$user_code\\.:\\s)(.+)\")\n",
    "\n",
    "re_udesc = re.compile(\"(?sm)\\$u(?:ser)?_?desc(?:ription)?\\.:\\s(.*?)(?=^\\$\\w+\\??\\.:?)\")\n",
    "re_udesc_alt = re.compile(\"(?sm)\\$u(?:ser)?_?desc(?:ription)?\\.:\\s(.+$)\")\n",
    "\n",
    "re_tdate = re.compile(\"(?:\\$tweet_date.:\\s)(.+)\")\n",
    "\n",
    "re_followers = re.compile(\"\\$(?:followerNo\\.|No.\\sfollowers\\.):\\s(\\d+\\.0)\")\n",
    "re_uverified = re.compile(\"\\$verified(?:\\_user)?\\?\\.:\\s(True|False)\")  \n",
    "    \n",
    "re_tweet = re.compile(\"(?sm)\\$tweet(?:\\_text)?.:\\s(.*?)(?=^\\$\\w+\\??\\.:?)\")\n",
    "re_tweet_alt = re.compile(\"(?sm)\\$tweet(?:\\_text)?\\.:\\s(.+$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T10:42:41.218761Z",
     "start_time": "2021-09-03T10:42:41.207298Z"
    }
   },
   "source": [
    "**Regex Explanation**\n",
    "\n",
    "1. re_uname\n",
    "> We create two groups, one non-capturing that will match the user name tag variation and other capturing group which will be the actual username text given by `(.+)`\n",
    "\n",
    "2. re_ucode \n",
    "> This again contains two groups, one non-capturing which returns match of user code tags and second capturing group which returns us the actual user code by `(.+)`\n",
    "\n",
    "3. re_udesc and re_udesc_alt\n",
    "> * There are two variations we have included to match the user description in the dataset. This we have done because user description tag has no fixed position in user instance. Mostly it is between other tags but in few instances it appears at the end. So as to match both according to condition (will see later) we take this approach. \n",
    "> * So, the `re_udesc` regex enables `(?sm)` tags where `m` modifier is multi line. We take up the data between between `\\$u(?:ser)?_?desc(?:ription)?\\.:\\s` (which is matching user description tag variations) and `(?=^\\$\\w+\\??\\.:?)` (which is matches any other when encountered) using the `(.*?)` lazily. The other variation `re_udesc_alt` is similar approach just it matches till end of the text `(.+$)` of a user instance data.\n",
    "\n",
    "4. re_tweet and re_tweet_alt\n",
    "> We use similar approach as used in user description, as for tweet text as well the position is not fixed in the user instance data.\n",
    "\n",
    "5. re_tdate\n",
    "> This matches the tweet date by using non-capturing to get `$tweet_date` tag and a capturing-group `(.+)` to get the actual date text.\n",
    "\n",
    "6. re_followers\n",
    "> Follower tag had two variations. We handle both and validates that it contains a numerical value with appended `.0` to it using `(\\d+\\.0)`\n",
    "\n",
    "7. re_uverified\n",
    "> Verified tag had two variations to it matches then and validates the tag value is `(True|False)`. Since `(True|False)` is capturing group we get the value of it for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Parsing and Operations\n",
    "\n",
    "The approach we use is briefly described below:\n",
    "\n",
    "For every instance of user data we capture the values of meta-data and related values and store it in variables. Since \"user description\" and \"user tweet\" had two cases because of its position variations, we match one first and if it return error/null while taking its `.group()` we look up the alternative regex and use it. \n",
    "\n",
    "Once everything is captured we write to a dictionary `tweets_dict` with `user_code` as key and well structured dictionary with data as its values. `user_tweet` will be a list of tweets as a user can have several tweets. This reminds us if need to update the metadate according to the recent tweet of the user. We wrote `compare_datetime(d1,d2)` function earlier, this will come to our rescue to update the values if `user_code` will already be present in dictionary.\n",
    "\n",
    "**Why we chose dictionary?**\n",
    "Dictionaries are implemented as a hash-map. So time-complexity is O(1) for the access which is much faster then O(n) for a list. As we have a large dataset if you use List it will take lot of time as compared to dictionary to perform the same operation. Another important factor to choose dictionary was that, since we had to keep users unique and append their tweets, dictionary was the first choice.\n",
    "\n",
    "Another important function we wrote was `handle_xml_chars(value, tag_type)` which according to the tag type replaces the xml special characters with required character/string **( This has been carefully observed with the sample output and have been decided as per it which tags requires which replacements )**. The ultimate purpose of doing this is that the xml we create should be a valid xml. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:50:26.649782Z",
     "start_time": "2021-09-03T12:50:26.329813Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_dict = {}\n",
    "\n",
    "# for each user instance in all users\n",
    "for tweet in tweet_info:\n",
    "\n",
    "    try:\n",
    "        # captures value of regex compiled earlier\n",
    "        user_code = re_ucode.search(tweet).group(1)\n",
    "        user_name = re_uname.search(tweet).group(1)\n",
    "        tweet_date = re_tdate.search(tweet).group(1)\n",
    "        user_followers = re_followers.search(tweet).group(1)\n",
    "        user_verfied = re_uverified.search(tweet).group(1)\n",
    "    except:\n",
    "        print('ERROR: Some meta data missing CHECK!!!!', user_name, user_code)\n",
    "\n",
    "\n",
    "    user_desc = re_udesc.search(tweet)    \n",
    "    user_tweet = re_tweet.search(tweet)\n",
    "\n",
    "\n",
    "    if (user_code and user_name and tweet_date and user_followers and user_verfied):\n",
    "        pass\n",
    "    else:\n",
    "        print(user_name)\n",
    "        print('ERROR: One of more then one tag seems to throw None value!!!')\n",
    "\n",
    "\n",
    "    try: \n",
    "        if user_desc:\n",
    "            user_desc = user_desc.group(1)\n",
    "        else:\n",
    "            user_desc_alt = re_udesc_alt.search(tweet)\n",
    "            user_desc = user_desc_alt.group(1)\n",
    "    except:\n",
    "        print('ERROR: user_desc missing !!!!')\n",
    "\n",
    "        \n",
    "    # user tweet    \n",
    "    try:\n",
    "        if user_tweet:\n",
    "            user_tweet = user_tweet.group(1)\n",
    "        else:\n",
    "            user_tweet_alt = re_tweet_alt.search(tweet)\n",
    "            user_tweet = user_tweet_alt.group(1)\n",
    "    except:\n",
    "        print('ERROR: user_tweet missing')\n",
    "\n",
    "\n",
    "    if (user_desc and user_tweet):\n",
    "        pass\n",
    "    else:\n",
    "        print(user_name)\n",
    "        print('ERROR: One of the (user_desc or user_tweet) seems to throw None value!!!')\n",
    "\n",
    "\n",
    "        \n",
    "    # Adding user data to dictionary according to condition    \n",
    "        \n",
    "    # if user already in dictionary    \n",
    "    if user_code in tweets_dict:\n",
    "        \n",
    "        old_tweet_datetime = tweets_dict[user_code]['tweet_date']\n",
    "        new_tweet_datetime = tweet_date\n",
    "        \n",
    "        # comparing date, if True returned: we will update the metadata to recent date\n",
    "        if compare_datetime(old_tweet_datetime, new_tweet_datetime):\n",
    "            tweets_dict[user_code]['name'] = handle_xml_chars(user_name, 'xml_name')\n",
    "            tweets_dict[user_code]['verified_user'] = user_verfied\n",
    "            tweets_dict[user_code]['user_description'] = handle_xml_chars(user_desc, 'xml_description')\n",
    "            tweets_dict[user_code]['no_followers'] = user_followers            \n",
    "        \n",
    "        # appending the tweet\n",
    "        tweets_dict[user_code]['user_tweet'].append(handle_xml_chars(user_tweet, 'xml_tweet'))\n",
    "    \n",
    "    # create a new user\n",
    "    else:\n",
    "        tweets_dict[user_code] = {\n",
    "            \"name\": handle_xml_chars(user_name,'xml_name'),\n",
    "            \"verified_user\": user_verfied,\n",
    "            \"user_description\": handle_xml_chars(user_desc, 'xml_description'),\n",
    "            \"no_followers\": user_followers,\n",
    "            \"tweet_date\": tweet_date,\n",
    "            \"user_tweet\": [handle_xml_chars(user_tweet, 'xml_tweet')]\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating XML string\n",
    "\n",
    "We create XML based string using fstring of python. This would lead us to have a full fledged text, which we can just write directly to a new `.xml` at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:50:26.689212Z",
     "start_time": "2021-09-03T12:50:26.652269Z"
    }
   },
   "outputs": [],
   "source": [
    "# xml's outermost tag open\n",
    "xml_string = '<users>'\n",
    "\n",
    "# for each user, create a xml string based block\n",
    "for user in tweets_dict:\n",
    "    \n",
    "    # joining each tweet and placing <tweet> at start of it, along with its closing tag\n",
    "    all_tweet_xml_str = ''.join([\"<tweet>\"+tweet.rstrip('\\n')+\"</tweet>\" for tweet in tweets_dict[user][\"user_tweet\"]])\n",
    "    \n",
    "    descript_xml_str = \"<user_description>\"+tweets_dict[user][\"user_description\"].rstrip('\\n')+\"</user_description>\"\n",
    "\n",
    "    xml_string += (\n",
    "\n",
    "        f'<user name=\\\"{tweets_dict[user][\"name\"]}\\\">'\n",
    "            f'<verified_user>{tweets_dict[user][\"verified_user\"]}</verified_user>'\n",
    "            f'{descript_xml_str}'\n",
    "            f'<no_followers>{tweets_dict[user][\"no_followers\"]}</no_followers>'\n",
    "            f'<tweets>'\n",
    "            f'{all_tweet_xml_str}'\n",
    "            f'</tweets>'\n",
    "        f'</user>'  \n",
    "    )\n",
    "    \n",
    "# xml's outermost tag close\n",
    "xml_string += '</users>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data contains lot of emojis, different languages we need to tackle that and make it parsable by xml\n",
    "\n",
    "**`encode()`**\n",
    "The encode() method encodes the string, using the specified encoding. If no encoding is specified, UTF-8 will be used.\n",
    "`encoding=\"ascii\"`: converts to ASCII encoding\n",
    "`errors=\"xmlcharrefreplace\"`: replaces the character with an xml character \n",
    "\n",
    "Finally we decode back to `utf-8` so as to get rid of `b'` enocoding output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:50:26.707849Z",
     "start_time": "2021-09-03T12:50:26.691580Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_xml_encoded = xml_string.encode(encoding=\"ascii\",errors=\"xmlcharrefreplace\").decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:50:26.713353Z",
     "start_time": "2021-09-03T12:50:26.709955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7416"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:45:47.462583Z",
     "start_time": "2021-09-03T12:45:47.457172Z"
    }
   },
   "source": [
    "> Unique users are `7416`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Writing XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:50:26.718957Z",
     "start_time": "2021-09-03T12:50:26.715017Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# writing .xml file with 'w' (write) mode\n",
    "\n",
    "with open('31072100.xml','w') as f:\n",
    "    f.write(final_xml_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required formatted XML file was created successfully as per the specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  W3 schools. Python String encode() Method. Retrieved from https://www.w3schools.com/python/ref_string_encode.asp\n",
    "*  W3 schools. Python String maketrans() Method. Retrieved from https://www.w3schools.com/python/ref_string_maketrans.asp\n",
    "* Code Beautify. XMLViewer. Used from https://codebeautify.org/xmlviewer\n",
    "* Ciro Santilli. How to validate very large XML files?. Answer on Stackoverflow. Retrieved from https://stackoverflow.com/questions/7528249/how-to-validate-very-large-xml-files\n",
    "* Regex 101. Used from https://regex101.com/\n",
    "* Steve Campbell. Guru99. Python RegEx: re.match(), re.search(), re.findall() with Example. Retrieved from https://www.guru99.com/python-regular-expressions-complete-tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
